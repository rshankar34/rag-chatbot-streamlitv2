"""RAG PDF Chatbot - Streamlit Cloud + AWS Edition"""
import streamlit as st
import uuid
import tempfile
import os
import sys
from pypdf import PdfReader

# LangChain imports
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

from aws_utils import S3Manager, DynamoDBChat

# Page config
st.set_page_config(page_title="RAG PDF Chatbot", page_icon="‚òÅÔ∏è", layout="wide")

# Initialize AWS services
@st.cache_resource
def init_aws():
    """Initialize AWS services with explicit region"""
    region = st.secrets.get("AWS_REGION", "eu-north-1")
    
    s3 = S3Manager(
        bucket_name=st.secrets["S3_BUCKET_NAME"],
        region=region
    )
    
    dynamo = DynamoDBChat(
        table_name="ChatHistory",
        region=region
    )
    
    return s3, dynamo

s3_manager, chat_db = init_aws()

# Initialize embeddings
@st.cache_resource
def init_embeddings():
    return HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )

embeddings = init_embeddings()

# Session state
if 'session_id' not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
if 'messages' not in st.session_state:
    history = chat_db.get_history(st.session_state.session_id)
    st.session_state.messages = [
        {"role": msg['role'], "content": msg['content']} 
        for msg in history
    ]
if 'vector_store' not in st.session_state:
    st.session_state.vector_store = None

# Helper functions
def process_pdf(file_bytes, filename):
    """Process PDF and add to vector store"""
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:
        tmp.write(file_bytes)
        tmp_path = tmp.name
    
    reader = PdfReader(tmp_path)
    text = ""
    for page in reader.pages:
        text += page.extract_text()
    
    os.unlink(tmp_path)
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = splitter.split_text(text)
    
    texts_with_meta = [(chunk, {"source": filename}) for chunk in chunks]
    
    if st.session_state.vector_store is None:
        st.session_state.vector_store = FAISS.from_texts(
            [t[0] for t in texts_with_meta],
            embeddings,
            metadatas=[t[1] for t in texts_with_meta]
        )
    else:
        new_store = FAISS.from_texts(
            [t[0] for t in texts_with_meta],
            embeddings,
            metadatas=[t[1] for t in texts_with_meta]
        )
        st.session_state.vector_store.merge_from(new_store)
    
    return True

def get_qa_chain():
    """Create QA chain with GPT-4o-mini"""
    try:
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.3,
            max_tokens=500,
            api_key=st.secrets["OPENAI_API_KEY"]
        )
        
        template = """Use the following context to answer the question. If you don't know the answer, say so clearly.

Context: {context}

Question: {question}

Answer:"""
        
        prompt = PromptTemplate(
            template=template, 
            input_variables=["context", "question"]
        )
        
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=st.session_state.vector_store.as_retriever(
                search_kwargs={"k": 4}
            ),
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt}
        )
        
        return qa_chain
        
    except Exception as e:
        st.error(f"Error creating QA chain: {e}")
        return None

# UI
st.title("‚òÅÔ∏è RAG PDF Chatbot")
st.caption("Upload PDFs, ask questions powered by AI")

# Sidebar
with st.sidebar:
    st.header("üìÑ PDF Management")
    
    uploaded_files = st.file_uploader(
        "Upload PDFs",
        type=['pdf'],
        accept_multiple_files=True
    )
    
    if st.button("üì§ Upload & Process", type="primary"):
        if uploaded_files:
            progress = st.progress(0)
            for idx, file in enumerate(uploaded_files):
                s3_key = s3_manager.upload_pdf(file.getvalue(), file.name)
                if s3_key:
                    process_pdf(file.getvalue(), file.name)
                progress.progress((idx + 1) / len(uploaded_files))
            st.success(f"‚úÖ Processed {len(uploaded_files)} PDF(s)!")
            progress.empty()
        else:
            st.warning("Upload PDFs first")
    
    st.divider()
    
    if st.button("üìã Show S3 PDFs"):
        pdfs = s3_manager.list_pdfs()
        if pdfs:
            st.write(f"**{len(pdfs)} PDF(s):**")
            for pdf in pdfs:
                st.text(f"‚Ä¢ {pdf.split('/')[-1]}")
        else:
            st.info("No PDFs yet")
    
    st.divider()
    
    if st.button("üóëÔ∏è Clear Chat"):
        chat_db.clear_history(st.session_state.session_id)
        st.session_state.messages = []
        st.success("Cleared!")
        st.rerun()

# Chat
st.subheader("üí¨ Chat")

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

if prompt := st.chat_input("Ask about your PDFs..."):
    if not st.session_state.vector_store:
        st.error("‚ö†Ô∏è Upload PDFs first!")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)
        
        chat_db.save_message(st.session_state.session_id, "user", prompt)
        
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                try:
                    qa_chain = get_qa_chain()
                    
                    if qa_chain is None:
                        st.error("‚ùå Failed to create QA chain.")
                        st.stop()
                    
                    result = qa_chain.invoke({"query": prompt})
                    answer = result['result']
                    
                    if 'source_documents' in result and result['source_documents']:
                        sources = set([doc.metadata.get('source', 'Unknown') 
                                     for doc in result['source_documents']])
                        answer += f"\n\nüìÑ *Sources: {', '.join(sources)}*"
                    
                    st.write(answer)
                    
                    chat_db.save_message(st.session_state.session_id, "assistant", answer)
                    st.session_state.messages.append({"role": "assistant", "content": answer})
                    
                except Exception as e:
                    st.error(f"‚ùå Error: {str(e)}")
